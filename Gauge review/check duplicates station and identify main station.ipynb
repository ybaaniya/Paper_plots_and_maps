{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from math import radians\n",
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives me a dataframe that check all teh duplicate station based on the distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Load the Excel file and preprocess the data.\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Clean and standardize latitude/longitude\n",
    "    # Convert to numeric, handling any errors\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing lat/long\n",
    "    df_clean = df.dropna(subset=['latitude', 'longitude'])\n",
    "    \n",
    "    # Create standardized text fields for matching\n",
    "    # Handle missing values by filling with empty strings instead of 'nan'\n",
    "    df_clean['gauge_name_clean'] = df_clean['Gauge Name/Number'].fillna('').astype(str).str.lower().str.strip()\n",
    "    df_clean['station_name_clean'] = df_clean['Station_name'].fillna('').astype(str).str.lower().str.strip()\n",
    "    df_clean['river_name_clean'] = df_clean['River Name (if reported)'].fillna('').astype(str).str.lower().str.strip()\n",
    "    \n",
    "    # Flag rows with valid metadata for smarter matching later\n",
    "    df_clean['has_gauge_name'] = ((df_clean['gauge_name_clean'] != '') & (df_clean['gauge_name_clean'] != 'nan'))\n",
    "    df_clean['has_station_name'] = ((df_clean['station_name_clean'] != '') & (df_clean['station_name_clean'] != 'nan'))\n",
    "    df_clean['has_river_name'] = ((df_clean['river_name_clean'] != '') & (df_clean['river_name_clean'] != 'nan'))\n",
    "    \n",
    "    print(f\"Original rows: {len(df)}, After cleaning: {len(df_clean)}\")\n",
    "    return df_clean\n",
    "\n",
    "def find_potential_duplicates(df, distance_threshold_km=0.2):\n",
    "    \"\"\"Find potential duplicate stations based on geographic proximity.\"\"\"\n",
    "    # Extract sources and create pairs of sources\n",
    "    sources = df['Name of Providing Entity'].unique()\n",
    "    source_pairs = list(itertools.combinations(sources, 2))\n",
    "    \n",
    "    # Initialize results container\n",
    "    potential_duplicates = []\n",
    "    \n",
    "    # For each pair of sources\n",
    "    for source1, source2 in source_pairs:\n",
    "        df1 = df[df['Name of Providing Entity'] == source1]\n",
    "        df2 = df[df['Name of Providing Entity'] == source2]\n",
    "        \n",
    "        print(f\"Comparing {len(df1)} stations from {source1} with {len(df2)} stations from {source2}\")\n",
    "        \n",
    "        # Convert threshold to radians (haversine_distances expects radians)\n",
    "        threshold_radians = distance_threshold_km / 6371.0  # Earth radius in km\n",
    "        \n",
    "        # For each station in the first source\n",
    "        for idx1, row1 in df1.iterrows():\n",
    "            # Convert coordinates to radians\n",
    "            lat1, lon1 = radians(row1['latitude']), radians(row1['longitude'])\n",
    "            \n",
    "            # For each station in the second source\n",
    "            for idx2, row2 in df2.iterrows():\n",
    "                # Convert coordinates to radians\n",
    "                lat2, lon2 = radians(row2['latitude']), radians(row2['longitude'])\n",
    "                \n",
    "                # Calculate haversine distance\n",
    "                distance = haversine_distances([[lat1, lon1]], [[lat2, lon2]])[0][0] * 6371.0  # Convert to km\n",
    "                \n",
    "                # If stations are close enough\n",
    "                if distance <= distance_threshold_km:\n",
    "                    # Calculate name similarity scores with handling for missing data\n",
    "                    gauge_name_sim = fuzz.ratio(row1['gauge_name_clean'], row2['gauge_name_clean']) if (row1['has_gauge_name'] and row2['has_gauge_name']) else 0\n",
    "                    station_name_sim = fuzz.ratio(row1['station_name_clean'], row2['station_name_clean']) if (row1['has_station_name'] and row2['has_station_name']) else 0\n",
    "                    river_name_sim = fuzz.ratio(row1['river_name_clean'], row2['river_name_clean']) if (row1['has_river_name'] and row2['has_river_name']) else 0\n",
    "                    \n",
    "                    # Count how many name fields are available for comparison\n",
    "                    available_comparisons = (\n",
    "                        int(row1['has_gauge_name'] and row2['has_gauge_name']) + \n",
    "                        int(row1['has_station_name'] and row2['has_station_name']) + \n",
    "                        int(row1['has_river_name'] and row2['has_river_name'])\n",
    "                    )\n",
    "                    \n",
    "                    # Create a match record\n",
    "                    match = {\n",
    "                        'idx1': idx1,\n",
    "                        'idx2': idx2,\n",
    "                        'source1': source1,\n",
    "                        'source2': source2,\n",
    "                        'gauge_name1': row1['Gauge Name/Number'],\n",
    "                        'gauge_name2': row2['Gauge Name/Number'],\n",
    "                        'station_name1': row1['Station_name'],\n",
    "                        'station_name2': row2['Station_name'],\n",
    "                        'river_name1': row1['River Name (if reported)'],\n",
    "                        'river_name2': row2['River Name (if reported)'],\n",
    "                        'country1': row1['Country'],\n",
    "                        'country2': row2['Country'],\n",
    "                        'lat1': row1['latitude'],\n",
    "                        'lon1': row1['longitude'],\n",
    "                        'lat2': row2['latitude'],\n",
    "                        'lon2': row2['longitude'],\n",
    "                        'distance_km': distance,\n",
    "                        'gauge_name_similarity': gauge_name_sim,\n",
    "                        'station_name_similarity': station_name_sim,\n",
    "                        'river_name_similarity': river_name_sim\n",
    "                    }\n",
    "                    potential_duplicates.append(match)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(potential_duplicates)\n",
    "    \n",
    "    # Calculate overall similarity score with adaptive weights based on available data\n",
    "    if not results_df.empty:\n",
    "        # Add count of available comparisons\n",
    "        results_df['available_comparisons'] = results_df.apply(\n",
    "            lambda x: ((x['gauge_name_similarity'] > 0) + \n",
    "                      (x['station_name_similarity'] > 0) + \n",
    "                      (x['river_name_similarity'] > 0)), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Calculate normalized weights based on what's available\n",
    "        results_df['overall_similarity'] = results_df.apply(\n",
    "            lambda x: (\n",
    "                (x['gauge_name_similarity'] * (0.3 if x['gauge_name_similarity'] > 0 else 0)) +\n",
    "                (x['station_name_similarity'] * (0.3 if x['station_name_similarity'] > 0 else 0)) +\n",
    "                (x['river_name_similarity'] * (0.2 if x['river_name_similarity'] > 0 else 0)) +\n",
    "                ((1 - x['distance_km'] / distance_threshold_km) * (0.2 + (0.8 - (x['available_comparisons'] * 0.2))))\n",
    "            ) / (0.2 + (x['available_comparisons'] * 0.2)),  # Normalize by the weights used\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Sort by overall similarity\n",
    "        results_df = results_df.sort_values('overall_similarity', ascending=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def classify_duplicates(duplicate_df, high_threshold=80, medium_threshold=60):\n",
    "    \"\"\"Classify potential duplicates into high, medium, and low confidence.\"\"\"\n",
    "    if duplicate_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Add confidence classification\n",
    "    duplicate_df['confidence'] = 'Low'\n",
    "    duplicate_df.loc[duplicate_df['overall_similarity'] >= medium_threshold, 'confidence'] = 'Medium'\n",
    "    duplicate_df.loc[duplicate_df['overall_similarity'] >= high_threshold, 'confidence'] = 'High'\n",
    "    \n",
    "    return duplicate_df\n",
    "\n",
    "def optimize_for_large_dataset(df, distance_threshold_km=0.2):\n",
    "    \"\"\"Optimized approach for very large datasets using geospatial partitioning.\"\"\"\n",
    "    \n",
    "    \n",
    "    # Extract sources\n",
    "    sources = df['Name of Providing Entity'].unique()\n",
    "    source_pairs = list(itertools.combinations(sources, 2))\n",
    "    \n",
    "    potential_duplicates = []\n",
    "    \n",
    "    for source1, source2 in source_pairs:\n",
    "        df1 = df[df['Name of Providing Entity'] == source1]\n",
    "        df2 = df[df['Name of Providing Entity'] == source2]\n",
    "        \n",
    "        print(f\"Comparing {len(df1)} stations from {source1} with {len(df2)} stations from {source2}\")\n",
    "        \n",
    "        # Create KD-Tree for faster spatial search\n",
    "        # Convert lat/lon to radians for proper distance calculation\n",
    "        coords2_rad = np.radians(df2[['latitude', 'longitude']].values)\n",
    "        tree = cKDTree(coords2_rad)\n",
    "        \n",
    "        # Threshold in radians\n",
    "        threshold_rad = distance_threshold_km / 6371.0\n",
    "        \n",
    "        # For each station in the first source\n",
    "        for idx1, row1 in df1.iterrows():\n",
    "            # Skip if coordinates are missing\n",
    "            if pd.isna(row1['latitude']) or pd.isna(row1['longitude']):\n",
    "                continue\n",
    "                \n",
    "            # Convert to radians\n",
    "            lat1_rad, lon1_rad = radians(row1['latitude']), radians(row1['longitude'])\n",
    "            \n",
    "            # Query the KD-Tree for neighbors within threshold\n",
    "            indices = tree.query_ball_point([lat1_rad, lon1_rad], threshold_rad)\n",
    "            \n",
    "            # Process matches\n",
    "            for i in indices:\n",
    "                idx2 = df2.iloc[i].name\n",
    "                row2 = df2.iloc[i]\n",
    "                \n",
    "                # Skip if countries are different\n",
    "                if row1['Country'] != row2['Country']:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate actual distance\n",
    "                lat2_rad, lon2_rad = radians(row2['latitude']), radians(row2['longitude'])\n",
    "                distance = haversine_distances([[lat1_rad, lon1_rad]], [[lat2_rad, lon2_rad]])[0][0] * 6371.0\n",
    "                \n",
    "                # Calculate similarity scores with handling for missing data\n",
    "                gauge_name_sim = fuzz.ratio(row1['gauge_name_clean'], row2['gauge_name_clean']) if (row1['has_gauge_name'] and row2['has_gauge_name']) else 0\n",
    "                station_name_sim = fuzz.ratio(row1['station_name_clean'], row2['station_name_clean']) if (row1['has_station_name'] and row2['has_station_name']) else 0\n",
    "                river_name_sim = fuzz.ratio(row1['river_name_clean'], row2['river_name_clean']) if (row1['has_river_name'] and row2['has_river_name']) else 0\n",
    "                \n",
    "                # Count how many name fields are available for comparison\n",
    "                available_comparisons = (\n",
    "                    int(row1['has_gauge_name'] and row2['has_gauge_name']) + \n",
    "                    int(row1['has_station_name'] and row2['has_station_name']) + \n",
    "                    int(row1['has_river_name'] and row2['has_river_name'])\n",
    "                )\n",
    "                \n",
    "                # Get byu_ids\n",
    "                byu_id1 = row1.get('byu_id', None)\n",
    "                byu_id2 = row2.get('byu_id', None)\n",
    "                \n",
    "                # Create match record\n",
    "                match = {\n",
    "                    'idx1': idx1,\n",
    "                    'idx2': idx2,\n",
    "                    'source1': source1,\n",
    "                    'source2': source2,\n",
    "                    'gauge_name1': row1['Gauge Name/Number'],\n",
    "                    'gauge_name2': row2['Gauge Name/Number'],\n",
    "                    'station_name1': row1['Station_name'],\n",
    "                    'station_name2': row2['Station_name'],\n",
    "                    'river_name1': row1['River Name (if reported)'],\n",
    "                    'river_name2': row2['River Name (if reported)'],\n",
    "                    'country1': row1['Country'],\n",
    "                    'country2': row2['Country'],\n",
    "                    'lat1': row1['latitude'],\n",
    "                    'lon1': row1['longitude'],\n",
    "                    'lat2': row2['latitude'],\n",
    "                    'lon2': row2['longitude'],\n",
    "                    'distance_km': distance,\n",
    "                    'gauge_name_similarity': gauge_name_sim,\n",
    "                    'station_name_similarity': station_name_sim,\n",
    "                    'river_name_similarity': river_name_sim,\n",
    "                    'byu_id1': byu_id1,\n",
    "                    'byu_id2': byu_id2,\n",
    "                    'available_comparisons': available_comparisons\n",
    "                }\n",
    "                potential_duplicates.append(match)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(potential_duplicates)\n",
    "    \n",
    "    # Calculate overall similarity score\n",
    "    if not results_df.empty:\n",
    "        results_df['overall_similarity'] = (\n",
    "            results_df['gauge_name_similarity'] * 0.3 + \n",
    "            results_df['station_name_similarity'] * 0.3 + \n",
    "            results_df['river_name_similarity'] * 0.2 + \n",
    "            (1 - results_df['distance_km'] / distance_threshold_km) * 0.2\n",
    "        )\n",
    "        \n",
    "        # Sort by overall similarity\n",
    "        results_df = results_df.sort_values('overall_similarity', ascending=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def mark_duplicates_in_dataframe(original_df, duplicate_pairs_df, confidence_threshold=60):\n",
    "    \"\"\"\n",
    "    Add 'duplicated' and 'primary_gauge' columns to the original dataframe.\n",
    "    The 'primary_gauge' column will contain the byu_ids of the duplicated rows.\n",
    "    \"\"\"\n",
    "    print(\"Marking duplicates in the original dataframe...\")\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    filtered_pairs = duplicate_pairs_df[\n",
    "        (duplicate_pairs_df['overall_similarity'] >= confidence_threshold)\n",
    "    ]\n",
    "    \n",
    "    if filtered_pairs.empty:\n",
    "        print(\"No duplicates found meeting the confidence threshold.\")\n",
    "        original_df['duplicated'] = False\n",
    "        original_df['primary_gauge'] = \"\"\n",
    "        return original_df\n",
    "    \n",
    "    print(f\"Number of duplicate pairs after filtering: {len(filtered_pairs)}\")\n",
    "    \n",
    "    # Create new columns in the original dataframe\n",
    "    original_df['duplicated'] = False\n",
    "    original_df['primary_gauge'] = \"\"\n",
    "    \n",
    "    # Dictionary to collect all duplicates for each row\n",
    "    duplicate_mapping = {}\n",
    "    \n",
    "    # Track which indices are processed\n",
    "    processed_indices = set()\n",
    "    \n",
    "    # Process each duplicate pair\n",
    "    pair_count = 0\n",
    "    for _, row in filtered_pairs.iterrows():\n",
    "        pair_count += 1\n",
    "        if pair_count % 1000 == 0:\n",
    "            print(f\"Processing pair {pair_count}/{len(filtered_pairs)}\")\n",
    "            \n",
    "        idx1, idx2 = int(row['idx1']), int(row['idx2'])\n",
    "        \n",
    "        # Debugging info\n",
    "        if pair_count <= 5:\n",
    "            print(f\"Pair {pair_count} indices: {idx1}, {idx2}\")\n",
    "            print(f\"These indices exist in original_df: {idx1 in original_df.index}, {idx2 in original_df.index}\")\n",
    "        \n",
    "        # Check if indices exist in the dataframe\n",
    "        if idx1 not in original_df.index or idx2 not in original_df.index:\n",
    "            print(f\"Warning: Index {idx1} or {idx2} not found in original dataframe\")\n",
    "            continue\n",
    "            \n",
    "        # Mark both rows as duplicates\n",
    "        original_df.loc[idx1, 'duplicated'] = True\n",
    "        original_df.loc[idx2, 'duplicated'] = True\n",
    "        \n",
    "        processed_indices.add(idx1)\n",
    "        processed_indices.add(idx2)\n",
    "        \n",
    "        # Get byu_ids and convert to strings\n",
    "        byu_id1 = str(row['byu_id1']) if not pd.isna(row['byu_id1']) else \"\"\n",
    "        byu_id2 = str(row['byu_id2']) if not pd.isna(row['byu_id2']) else \"\"\n",
    "        \n",
    "        # Add to duplicate mapping\n",
    "        if idx1 not in duplicate_mapping:\n",
    "            duplicate_mapping[idx1] = set()\n",
    "        if idx2 not in duplicate_mapping:\n",
    "            duplicate_mapping[idx2] = set()\n",
    "            \n",
    "        # Add the other's byu_id to each row's set\n",
    "        if byu_id2:\n",
    "            duplicate_mapping[idx1].add(byu_id2)\n",
    "        if byu_id1:\n",
    "            duplicate_mapping[idx2].add(byu_id1)\n",
    "    \n",
    "    # Fill the primary_gauge column with collected byu_ids\n",
    "    for idx, byu_ids in duplicate_mapping.items():\n",
    "        if byu_ids:  # Only set if there are actual byu_ids\n",
    "            original_df.loc[idx, 'primary_gauge'] = \",\".join(str(id) for id in byu_ids)\n",
    "    \n",
    "    # Count duplicates\n",
    "    duplicate_count = original_df['duplicated'].sum()\n",
    "    \n",
    "    print(f\"Found {duplicate_count} duplicate stations across {len(processed_indices)} unique indices\")\n",
    "    return original_df\n",
    "\n",
    "def main(file_path, distance_threshold_km=0.2, use_optimized=True, confidence_threshold=0):\n",
    "    \"\"\"Main function to process the file and find duplicates.\"\"\"\n",
    "    # Load the original data without preprocessing\n",
    "    print(\"Loading original data...\")\n",
    "    original_df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Load and preprocess data for comparison\n",
    "    print(\"Preprocessing data for comparison...\")\n",
    "    df = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    # Find potential duplicates\n",
    "    print(\"Finding potential duplicates...\")\n",
    "    if use_optimized and len(df) > 10000:\n",
    "        print(f\"Using optimized approach for large dataset with {len(df)} rows\")\n",
    "        duplicates_df = optimize_for_large_dataset(df, distance_threshold_km)\n",
    "    else:\n",
    "        duplicates_df = find_potential_duplicates(df, distance_threshold_km)\n",
    "    \n",
    "    print(f\"Found {len(duplicates_df)} potential duplicate pairs before classification\")\n",
    "    \n",
    "    # Classify duplicates\n",
    "    classified_df = classify_duplicates(duplicates_df)\n",
    "    print(f\"After classification: {len(classified_df)} duplicate pairs\")\n",
    "    \n",
    "    if not classified_df.empty:\n",
    "        # Mark duplicates in the original dataframe\n",
    "        result_df = mark_duplicates_in_dataframe(original_df, classified_df, confidence_threshold)\n",
    "        \n",
    "        # Save the updated original dataframe\n",
    "        output_file = file_path.replace('.xlsx', '_with_duplicates_markedbgnbnnbmnbmnbmbnmchcjgcjgx.xlsx')\n",
    "        result_df.to_excel(output_file, index=False)\n",
    "        print(f\"Updated dataframe saved to {output_file}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nDuplicate Summary:\")\n",
    "        print(f\"Total duplicate pairs found: {len(classified_df)}\")\n",
    "        print(f\"Total duplicate stations marked: {result_df['duplicated'].sum()}\")\n",
    "    else:\n",
    "        print(\"No potential duplicates found.\")\n",
    "        # Still add the columns but mark everything as non-duplicate\n",
    "        original_df['duplicated'] = False\n",
    "        original_df['primary_gauge'] = \"\"\n",
    "        \n",
    "        output_file = file_path.replace('.xlsx', '_with_duplicates_marked.xlsx')\n",
    "        original_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    return original_df\n",
    "# Usage\n",
    "duplicate_df = main('/Users/yubinbaaniya/Downloads/combined_file_with_metadata jan9 with random identification_3.xlsx', distance_threshold_km=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on above created dataframe it'll check which gaueg among the duplicated one is \"the\" main file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_main_gauges(file_path):\n",
    "    \"\"\"\n",
    "    Identifies the main gauge between duplicates based on source priority.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the Excel file with duplicates marked\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with added \"main gauge\" column\n",
    "    \"\"\"\n",
    "    print(\"Loading Excel file...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Create the main gauge column with default empty values\n",
    "    df['main gauge'] = \"\"\n",
    "    \n",
    "    # Define source priorities (lower value = higher priority)\n",
    "    source_priorities = {\n",
    "        'CARAVAN': 2,  # Lowest priority\n",
    "        'GRDC': 1,     # Second lowest priority\n",
    "        # All other sources will get priority 0 (highest)\n",
    "    }\n",
    "    \n",
    "    # Process each row with other_gauge values\n",
    "    processed_count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        # If other_gauge is empty, skip\n",
    "        if pd.isna(row['primary_gauge']) or row['primary_gauge'] == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Get the byu_id of the current row\n",
    "        current_byu_id = row['byu_id']\n",
    "        if pd.isna(current_byu_id):\n",
    "            continue\n",
    "            \n",
    "        # Get the list of other gauge IDs\n",
    "        other_gauge_ids = str(row['primary_gauge']).split(',')\n",
    "        \n",
    "        # For tracking the best priority among duplicates\n",
    "        best_priority = float('inf')\n",
    "        best_id = None\n",
    "        \n",
    "        # Get priority of the current row\n",
    "        current_source = row['Name of Providing Entity']\n",
    "        current_priority = source_priorities.get(current_source, 0)  # Default to 0 (highest) if not in dict\n",
    "        \n",
    "        # Track all related byu_ids (current one + all in other_gauge)\n",
    "        all_related_ids = [str(current_byu_id)] + other_gauge_ids\n",
    "        all_related_ids = [x.strip() for x in all_related_ids if x.strip()]\n",
    "        \n",
    "        # Record the best ID so far (which might be the current row)\n",
    "        if current_priority < best_priority:\n",
    "            best_priority = current_priority\n",
    "            best_id = current_byu_id\n",
    "        \n",
    "        # Find rows for each other gauge ID and compare priorities\n",
    "        for gauge_id in other_gauge_ids:\n",
    "            if gauge_id.strip() == \"\":\n",
    "                continue\n",
    "                \n",
    "            # Find rows with matching byu_id\n",
    "            matching_rows = df[df['byu_id'].astype(str) == gauge_id.strip()]\n",
    "            \n",
    "            if not matching_rows.empty:\n",
    "                # Get the first matching row (there should be only one)\n",
    "                match_row = matching_rows.iloc[0]\n",
    "                match_source = match_row['Name of Providing Entity']\n",
    "                match_priority = source_priorities.get(match_source, 0)  # Default to 0 if not in dict\n",
    "                \n",
    "                # Check if this is a better priority\n",
    "                if match_priority < best_priority:\n",
    "                    best_priority = match_priority\n",
    "                    best_id = match_row['byu_id']\n",
    "        \n",
    "        # Mark the main gauge\n",
    "        if best_id is not None:\n",
    "            # Find all rows with byu_id in the related IDs list\n",
    "            for rel_id in all_related_ids:\n",
    "                matching_rows = df[df['byu_id'].astype(str) == rel_id.strip()]\n",
    "                if not matching_rows.empty:\n",
    "                    for match_idx in matching_rows.index:\n",
    "                        # Is this the row with the best priority?\n",
    "                        if str(df.loc[match_idx, 'byu_id']) == str(best_id):\n",
    "                            df.loc[match_idx, 'main gauge'] = \"1\"\n",
    "        \n",
    "        processed_count += 1\n",
    "        if processed_count % 100 == 0:\n",
    "            print(f\"Processed {processed_count} rows with other_gauge values\")\n",
    "    \n",
    "    # Save the updated file\n",
    "    output_file = file_path.replace('.xlsx', '_with_main_gauge.xlsx')\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Completed processing. Main gauge identified for {df['main gauge'].value_counts().get('1', 0)} rows.\")\n",
    "    print(f\"Updated file saved to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_result = identify_main_gauges('/Users/yubinbaaniya/Downloads/combined_file_with_metadata jan9 with random identification_3_with_duplicates_markedbgnbnnbmnbmnbmbnmchcjgcjgx.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
