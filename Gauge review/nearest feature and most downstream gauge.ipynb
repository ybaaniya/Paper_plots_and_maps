{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest stream LINKNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = '/Users/yubinbaaniya/Documents/GAUGE REVIEW/master_file.xlsx'\n",
    "gpkg_file = '/Users/yubinbaaniya/Library/CloudStorage/Box-Box/master thesis and what not/Geoglows AWS files except VPU/global_streams_simplified.gpkg'\n",
    "output_excel = '/Users/yubinbaaniya/Documents/GAUGE REVIEW/other calculation_ on master file/master_file_with_nearest.xlsx'\n",
    "\n",
    "#Read and Filter Excel Data\n",
    "\n",
    "try:\n",
    "    # Read Excel file into a pandas DataFrame\n",
    "    df_excel = pd.read_excel(excel_file)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error reading Excel file: {e}\")\n",
    "\n",
    "# Check for required latitude and longitude columns\n",
    "for col in ['latitude', 'longitude']:\n",
    "    if col not in df_excel.columns:\n",
    "        raise ValueError(f\"Excel file must contain a '{col}' column.\")\n",
    "\n",
    "# Filter out rows missing either latitude or longitude\n",
    "df_excel_filtered = df_excel.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Filter rows where \"TDX-Hydro Paired Rivers\" is 0, empty, or null\n",
    "if 'TDX-Hydro Paired Rivers' not in df_excel_filtered.columns:\n",
    "    raise ValueError(\"Excel file must contain a 'TDX-Hydro Paired Rivers' column.\")\n",
    "\n",
    "df_excel_filtered = df_excel_filtered[\n",
    "    (df_excel_filtered['TDX-Hydro Paired Rivers'].isnull()) |\n",
    "    (df_excel_filtered['TDX-Hydro Paired Rivers'] == 0)\n",
    "]\n",
    "\n",
    "\n",
    "#Convert Filtered Points to a GeoDataFrame\n",
    "# Create a geometry column from longitude and latitude\n",
    "gdf_points = gpd.GeoDataFrame(\n",
    "    df_excel_filtered,\n",
    "    geometry=gpd.points_from_xy(df_excel_filtered['longitude'], df_excel_filtered['latitude']),\n",
    "    crs=\"EPSG:4326\"  # original coordinate system\n",
    ")\n",
    "\n",
    "\n",
    "#Read Stream Network from GeoPackage\n",
    "\n",
    "try:\n",
    "    gdf_streams = gpd.read_file(gpkg_file)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error reading GeoPackage file: {e}\")\n",
    "\n",
    "# Check for the necessary 'LINKNO' column in stream network data\n",
    "if \"LINKNO\" not in gdf_streams.columns:\n",
    "    raise ValueError(\"GeoPackage file must contain a 'LINKNO' column.\")\n",
    "\n",
    "\n",
    "# Reproject Data for Accurate Distance Calculation\n",
    "# For distance calculations in meters, reproject both datasets to a projected CRS.\n",
    "# EPSG:3857 (Web Mercator) is used here for simplicity.\n",
    "gdf_points_proj = gdf_points.to_crs(epsg=3857)\n",
    "gdf_streams_proj = gdf_streams.to_crs(epsg=3857)\n",
    "\n",
    "#Find the Nearest Stream Segment for Each Point\n",
    "# Using geopandas.sjoin_nearest\n",
    "# this function will add the nearest 'LINKNO' and a 'distance' column (in meters)\n",
    "try:\n",
    "    # We perform a spatial join using only the necessary columns from the stream network\n",
    "    gdf_nearest = gpd.sjoin_nearest(\n",
    "        gdf_points_proj, \n",
    "        gdf_streams_proj[['LINKNO', 'geometry']], \n",
    "        how=\"left\", \n",
    "        distance_col=\"distance\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error during spatial join: {e}\")\n",
    "\n",
    "# (Optional) Reproject Result Back to EPSG:4326\n",
    "\n",
    "gdf_nearest = gdf_nearest.to_crs(epsg=4326)\n",
    "\n",
    "# Save the Output\n",
    "# For output, we include all original columns plus the nearest LINKNO and distance.\n",
    "# We drop the geometry column for Excel export\n",
    "try:\n",
    "    gdf_nearest.drop(columns=\"geometry\").to_excel(output_excel, index=False)\n",
    "    print(f\"Output saved to {output_excel}\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error saving the output Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most downstream LINKNO to make area covered by the gauge map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets\n",
    "network_df = pd.read_parquet('/Users/yubinbaaniya/Library/CloudStorage/Box-Box/master thesis and what not/Geoglows AWS files except VPU/v2-master-table.parquet') #contain LINKNO AND DSLINKNO to make a network\n",
    "gauge_df = pd.read_excel('/Users/yubinbaaniya/Documents/GAUGE REVIEW/other calculation_ on master file/master_file_with_nearest_for_all_station&area.xlsx')\n",
    "\n",
    "# Create a new column for the downstream flag; default is blank.\n",
    "gauge_df['DownstreamFlag'] = \"\"\n",
    "\n",
    "# Process each river system (VPUCode) separately.\n",
    "for vpu in gauge_df['VPUCode'].dropna().unique():\n",
    "    print(f\"Processing VPU: {vpu}\")\n",
    "    \n",
    "    # Subset the gauge and network data for the current VPUCode.\n",
    "    gauge_subset = gauge_df[gauge_df['VPUCode'] == vpu]\n",
    "    network_subset = network_df[network_df['VPUCode'] == vpu]\n",
    "    \n",
    "    # Build the directed graph for the river network.\n",
    "    # Nodes are defined by 'LINKNO' and edges are from 'LINKNO' to 'DSLINKNO'\n",
    "    G = nx.DiGraph()\n",
    "    for idx, row in network_subset.iterrows():\n",
    "        linkno = row['LINKNO']\n",
    "        dslinkno = row['DSLINKNO']\n",
    "        if pd.notnull(linkno):\n",
    "            G.add_node(linkno)\n",
    "        if pd.notnull(linkno) and pd.notnull(dslinkno):\n",
    "            G.add_edge(linkno, dslinkno)\n",
    "    \n",
    "    # Filter gauge rows with a valid LINKNO_excel for connectivity analysis.\n",
    "    candidates = gauge_subset[pd.notnull(gauge_subset['LINKNO_excel'])]\n",
    "    candidate_ids = candidates['LINKNO_excel'].tolist()\n",
    "    print(f\"  Found {len(candidate_ids)} candidate gauge(s) for VPU {vpu}\")\n",
    "    \n",
    "    # For each candidate gauge, determine its immediate downstream candidate gauge.\n",
    "    downstream_mapping = {}  # key: candidate gauge id, value: downstream gauge id or \"downstream\"\n",
    "    for candidate in candidate_ids:\n",
    "        # If the candidate is not in the network graph, leave flag blank.\n",
    "        if candidate not in G:\n",
    "            downstream_mapping[candidate] = \"\"\n",
    "            continue\n",
    "        \n",
    "        # Get all nodes downstream of the candidate.\n",
    "        descendants = nx.descendants(G, candidate)\n",
    "        # Filter to keep only those that are candidate gauges.\n",
    "        downstream_candidates = [d for d in descendants if d in candidate_ids]\n",
    "        \n",
    "        if not downstream_candidates:\n",
    "            # No downstream gauge candidate found: mark as \"downstream\".\n",
    "            downstream_mapping[candidate] = \"downstream\"\n",
    "        else:\n",
    "            # Determine the immediate downstream candidate gauge.\n",
    "            min_distance = np.inf\n",
    "            immediate_downstream = None\n",
    "            for d in downstream_candidates:\n",
    "                try:\n",
    "                    distance = nx.shortest_path_length(G, source=candidate, target=d)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    immediate_downstream = d\n",
    "                elif distance == min_distance:\n",
    "                    # If there is a tie, use DSContArea (if available) as tie-breaker.\n",
    "                    ds_area_current = candidates[candidates['LINKNO_excel'] == immediate_downstream]['DSContArea']\n",
    "                    ds_area_candidate = candidates[candidates['LINKNO_excel'] == d]['DSContArea']\n",
    "                    if (not ds_area_current.empty) and (not ds_area_candidate.empty):\n",
    "                        if ds_area_candidate.iloc[0] > ds_area_current.iloc[0]:\n",
    "                            immediate_downstream = d\n",
    "            downstream_mapping[candidate] = immediate_downstream\n",
    "    \n",
    "    print(f\"  Downstream mapping for VPU {vpu}: {downstream_mapping}\")\n",
    "    \n",
    "    # Update the original gauge dataset.\n",
    "    for idx, row in gauge_subset.iterrows():\n",
    "        candidate = row['LINKNO_excel']\n",
    "        if pd.notnull(candidate) and candidate in downstream_mapping:\n",
    "            gauge_df.loc[idx, 'DownstreamFlag'] = downstream_mapping[candidate]\n",
    "    \n",
    "    print(f\"Finished processing VPU: {vpu}\\n\")\n",
    "\n",
    "# Write the output to a new Excel file.\n",
    "output_path = '/Users/yubinbaaniya/Documents/GAUGE REVIEW/other calculation_ on master file/master_file_with_most downstream.xlsx'\n",
    "gauge_df.to_excel(output_path, index=False)\n",
    "print(f\"Output written to '{output_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
